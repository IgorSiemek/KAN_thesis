{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Cell 1: Import Libraries and Set Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error,\n",
    "    max_error\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def seed_everything(seed=4):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "# Check Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 2: Define Symbolic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple symbolic functions\n",
    "def symbolic_function_1(x):\n",
    "    return np.sin(5 * x) * np.exp(-x**2)\n",
    "\n",
    "def symbolic_function_2(x):\n",
    "    return np.where(x < 0, np.sin(5 * x), np.cos(5 * x)) * np.exp(-x**2)\n",
    "\n",
    "def symbolic_function_3(x):\n",
    "    return np.abs(np.sin(5 * x)) * np.exp(-x**2)\n",
    "\n",
    "def symbolic_function_4(x):\n",
    "    return np.sin(10 * x) * np.exp(-x**2)\n",
    "\n",
    "def symbolic_function_5(x):\n",
    "    return np.sin(15 * x) * np.exp(-x**2)\n",
    "\n",
    "def symbolic_function_6(x):\n",
    "    return np.piecewise(x, [x < 0, x >= 0], [lambda x: np.sin(5 * x), lambda x: np.sin(5 * x) + 1]) * np.exp(-x**2)\n",
    "\n",
    "# Additional Function Classes\n",
    "\n",
    "# Polynomial Functions\n",
    "def polynomial_function_1(x):\n",
    "    # f(x) = x^2\n",
    "    return x**2\n",
    "\n",
    "def polynomial_function_2(x):\n",
    "    # f(x) = x^3 - 2x + 1\n",
    "    return x**3 - 2*x + 1\n",
    "\n",
    "# Exponential Functions\n",
    "def exponential_function_1(x):\n",
    "    # f(x) = e^x\n",
    "    return np.exp(x)\n",
    "\n",
    "def exponential_function_2(x):\n",
    "    # f(x) = 2e^(0.5x)\n",
    "    return 2 * np.exp(0.5 * x)\n",
    "\n",
    "# Logarithmic Functions\n",
    "def logarithmic_function_1(x):\n",
    "    # f(x) = ln(|x| + 1)\n",
    "    return np.log(np.abs(x) + 1)\n",
    "\n",
    "def logarithmic_function_2(x):\n",
    "    # f(x) = log10(|x| + 1)\n",
    "    return np.log10(np.abs(x) + 1)\n",
    "\n",
    "# Extended Trigonometric Functions\n",
    "def trigonometric_function_3(x):\n",
    "    # f(x) = tan(x) * exp(-x^2)\n",
    "    return np.tan(x) * np.exp(-x**2)\n",
    "\n",
    "def trigonometric_function_4(x):\n",
    "    # f(x) = cot(x) * exp(-x^2)\n",
    "    return (1 / np.tan(x)) * np.exp(-x**2)\n",
    "\n",
    "# Step Functions\n",
    "def step_function_1(x):\n",
    "    # Heaviside function: f(x) = 0 dla x < 0, 1 dla x >= 0\n",
    "    return np.where(x < 0, 0, 1)\n",
    "\n",
    "def step_function_2(x):\n",
    "    # Step function z przesuniÄ™ciem: f(x) = 0 dla x < -1, 1 dla x >= -1\n",
    "    return np.where(x < -1, 0, 1)\n",
    "\n",
    "# Uniform Functions\n",
    "def uniform_function_1(x):\n",
    "    # f(x) = 5\n",
    "    return 5 * np.ones_like(x)\n",
    "\n",
    "def uniform_function_2(x):\n",
    "    # f(x) = 3x + 2 + losowy szum\n",
    "    return 3 * x + 2 + np.random.uniform(-0.5, 0.5, size=x.shape)\n",
    "\n",
    "# Dictionary of functions for easy access\n",
    "functions = {\n",
    "    'Function 1': symbolic_function_1,\n",
    "    'Function 2': symbolic_function_2,\n",
    "    'Function 3': symbolic_function_3,\n",
    "    'Function 4': symbolic_function_4,\n",
    "    'Function 5': symbolic_function_5,\n",
    "    'Function 6': symbolic_function_6,\n",
    "    \n",
    "    # Polynomial Functions\n",
    "    'Polynomial 1': polynomial_function_1,\n",
    "    'Polynomial 2': polynomial_function_2,\n",
    "    \n",
    "    # Exponential Functions\n",
    "    'Exponential 1': exponential_function_1,\n",
    "    'Exponential 2': exponential_function_2,\n",
    "    \n",
    "    # Logarithmic Functions\n",
    "    'Logarithmic 1': logarithmic_function_1,\n",
    "    'Logarithmic 2': logarithmic_function_2,\n",
    "    \n",
    "    # Extended Trigonometric Functions\n",
    "    'Trigonometric 3': trigonometric_function_3,\n",
    "    'Trigonometric 4': trigonometric_function_4,\n",
    "    \n",
    "    # Step Functions\n",
    "    'Step 1': step_function_1,\n",
    "    'Step 2': step_function_2,\n",
    "    \n",
    "    # Uniform Functions\n",
    "    'Uniform 1': uniform_function_1,\n",
    "    'Uniform 2': uniform_function_2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3: Data Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation Function with Output Normalization\n",
    "def generate_data(symbolic_function, n_samples=1000):\n",
    "    x_values = np.linspace(-2, 2, n_samples)\n",
    "    np.random.shuffle(x_values)\n",
    "    y_values = symbolic_function(x_values)\n",
    "\n",
    "    # Split into train, validation, and test sets\n",
    "    train_split = int(0.7 * n_samples)\n",
    "    val_split = int(0.85 * n_samples)\n",
    "\n",
    "    x_train = x_values[:train_split]\n",
    "    y_train = y_values[:train_split]\n",
    "\n",
    "    x_val = x_values[train_split:val_split]\n",
    "    y_val = y_values[train_split:val_split]\n",
    "\n",
    "    x_test = x_values[val_split:]\n",
    "    y_test = y_values[val_split:]\n",
    "\n",
    "    # Normalize inputs\n",
    "    x_mean = x_train.mean()\n",
    "    x_std = x_train.std()\n",
    "    x_train = (x_train - x_mean) / x_std\n",
    "    x_val = (x_val - x_mean) / x_std\n",
    "    x_test = (x_test - x_mean) / x_std\n",
    "\n",
    "    # Normalize outputs\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_train = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_val = y_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
    "    y_test = y_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Convert to tensors\n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    x_val_tensor = torch.tensor(x_val, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    x_test_tensor = torch.tensor(x_test, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, x_test, y_test, y_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 4: Define the MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=10, output_dim=1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 5: Define the BSpline Activation and KAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSplineActivation(nn.Module):\n",
    "    def __init__(self, num_parameters=20, init_range=(-1, 1)):\n",
    "        super(BSplineActivation, self).__init__()\n",
    "        # Initialize control points\n",
    "        self.control_points = nn.Parameter(torch.linspace(init_range[0], init_range[1], num_parameters))\n",
    "        # Initialize weights with smaller variance to stabilize training\n",
    "        self.weights = nn.Parameter(torch.randn(num_parameters) * 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the B-spline approximation using linear interpolation\n",
    "        x_clipped = torch.clamp(x, self.control_points[0], self.control_points[-1])\n",
    "        indices = torch.bucketize(x_clipped, self.control_points)\n",
    "        indices = torch.clamp(indices, 1, len(self.control_points) - 1)\n",
    "\n",
    "        x0 = self.control_points[indices - 1]\n",
    "        x1 = self.control_points[indices]\n",
    "        y0 = self.weights[indices - 1]\n",
    "        y1 = self.weights[indices]\n",
    "\n",
    "        slope = (y1 - y0) / (x1 - x0 + 1e-6)\n",
    "        output = y0 + slope * (x_clipped - x0)\n",
    "        return output\n",
    "\n",
    "# KAN Model Definition with Increased Spline Points and Dropout\n",
    "class KAN(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=10, output_dim=1, num_spline_points=30):\n",
    "        super(KAN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.act1 = BSplineActivation(num_parameters=num_spline_points)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.act2 = BSplineActivation(num_parameters=num_spline_points)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.act3 = BSplineActivation(num_parameters=num_spline_points)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 6: Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation Functions\n",
    "def train_model(model, optimizer, criterion, train_loader, val_loader, num_epochs=200, patience=50):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc='Training'):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            # Gradient Clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "\n",
    "        scheduler.step(epoch_val_loss)\n",
    "\n",
    "        # Early Stopping\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Load the best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_model(model, criterion, test_loader, y_scaler):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            preds.append(outputs.cpu().numpy())\n",
    "            trues.append(targets.cpu().numpy())\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "    preds = np.concatenate(preds).flatten()\n",
    "    trues = np.concatenate(trues).flatten()\n",
    "\n",
    "    # Inverse transform predictions and true values\n",
    "    preds = y_scaler.inverse_transform(preds.reshape(-1, 1)).flatten()\n",
    "    trues = y_scaler.inverse_transform(trues.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Calculating additional metrics\n",
    "    mae = mean_absolute_error(trues, preds)\n",
    "    r2 = r2_score(trues, preds)\n",
    "    mape = mean_absolute_percentage_error(trues, preds)\n",
    "    max_err = max_error(trues, preds)\n",
    "\n",
    "    return test_loss, mae, r2, mape, max_err, preds, trues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 7: Main Loop to Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define criterion\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Store results for all functions\n",
    "results = {}\n",
    "\n",
    "for func_name, func in functions.items():\n",
    "    print(f\"\\nProcessing {func_name}...\")\n",
    "\n",
    "    # Generate data\n",
    "    train_loader, val_loader, test_loader, x_test, y_test, y_scaler = generate_data(func)\n",
    "\n",
    "    # Initialize models\n",
    "    mlp_model = MLP().to(device)\n",
    "    kan_model = KAN().to(device)\n",
    "\n",
    "    # Optimizers with adjusted learning rates and weight decay\n",
    "    mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=0.01)\n",
    "    kan_optimizer = optim.Adam(kan_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "    # Train MLP Model\n",
    "    print(\"Training MLP Model...\")\n",
    "    mlp_train_losses, mlp_val_losses = train_model(\n",
    "        mlp_model, mlp_optimizer, criterion, train_loader, val_loader\n",
    "    )\n",
    "    mlp_test_loss, mlp_mae, mlp_r2, mlp_mape, mlp_max_err, mlp_preds, mlp_trues = evaluate_model(\n",
    "        mlp_model, criterion, test_loader, y_scaler\n",
    "    )\n",
    "\n",
    "    # Train KAN Model\n",
    "    print(\"Training KAN Model...\")\n",
    "    kan_train_losses, kan_val_losses = train_model(\n",
    "        kan_model, kan_optimizer, criterion, train_loader, val_loader\n",
    "    )\n",
    "    kan_test_loss, kan_mae, kan_r2, kan_mape, kan_max_err, kan_preds, kan_trues = evaluate_model(\n",
    "        kan_model, criterion, test_loader, y_scaler\n",
    "    )\n",
    "\n",
    "    # Store results\n",
    "    results[func_name] = {\n",
    "        'MLP': {\n",
    "            'train_losses': mlp_train_losses,\n",
    "            'val_losses': mlp_val_losses,\n",
    "            'test_loss': mlp_test_loss,\n",
    "            'mae': mlp_mae,\n",
    "            'r2': mlp_r2,\n",
    "            'mape': mlp_mape,\n",
    "            'max_error': mlp_max_err,\n",
    "            'preds': mlp_preds,\n",
    "            'trues': mlp_trues,\n",
    "        },\n",
    "        'KAN': {\n",
    "            'train_losses': kan_train_losses,\n",
    "            'val_losses': kan_val_losses,\n",
    "            'test_loss': kan_test_loss,\n",
    "            'mae': kan_mae,\n",
    "            'r2': kan_r2,\n",
    "            'mape': kan_mape,\n",
    "            'max_error': kan_max_err,\n",
    "            'preds': kan_preds,\n",
    "            'trues': kan_trues,\n",
    "        },\n",
    "        'x_test': x_test,\n",
    "        'y_test': y_test,\n",
    "    }\n",
    "\n",
    "    # Plotting results\n",
    "    print(f\"\\nResults for {func_name}:\")\n",
    "\n",
    "    # Extract data\n",
    "    mlp_train_losses = results[func_name]['MLP']['train_losses']\n",
    "    mlp_val_losses = results[func_name]['MLP']['val_losses']\n",
    "    kan_train_losses = results[func_name]['KAN']['train_losses']\n",
    "    kan_val_losses = results[func_name]['KAN']['val_losses']\n",
    "\n",
    "    # Plot Training and Validation Losses\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(mlp_train_losses, label='MLP Training Loss')\n",
    "    plt.plot(mlp_val_losses, label='MLP Validation Loss')\n",
    "    plt.plot(kan_train_losses, label='KAN Training Loss')\n",
    "    plt.plot(kan_val_losses, label='KAN Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title(f'Training and Validation Losses for {func_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Test Metrics\n",
    "    print(f\"MLP Test MSE Loss: {results[func_name]['MLP']['test_loss']:.6f}\")\n",
    "    print(f\"MLP Test MAE: {results[func_name]['MLP']['mae']:.6f}\")\n",
    "    print(f\"MLP Test R^2 Score: {results[func_name]['MLP']['r2']:.6f}\")\n",
    "    print(f\"MLP Test MAPE: {results[func_name]['MLP']['mape']:.2f}%\")\n",
    "    print(f\"MLP Test Max Error: {results[func_name]['MLP']['max_error']:.6f}\")\n",
    "\n",
    "    print(f\"KAN Test MSE Loss: {results[func_name]['KAN']['test_loss']:.6f}\")\n",
    "    print(f\"KAN Test MAE: {results[func_name]['KAN']['mae']:.6f}\")\n",
    "    print(f\"KAN Test R^2 Score: {results[func_name]['KAN']['r2']:.6f}\")\n",
    "    print(f\"KAN Test MAPE: {results[func_name]['KAN']['mape']:.2f}%\")\n",
    "    print(f\"KAN Test Max Error: {results[func_name]['KAN']['max_error']:.6f}\")\n",
    "\n",
    "    # Plot Predictions vs True Function\n",
    "    x_test = results[func_name]['x_test']\n",
    "    y_test = results[func_name]['y_test']\n",
    "    mlp_preds = results[func_name]['MLP']['preds']\n",
    "    kan_preds = results[func_name]['KAN']['preds']\n",
    "\n",
    "    # Sort for plotting\n",
    "    sorted_indices = np.argsort(x_test)\n",
    "    x_test_sorted = x_test[sorted_indices]\n",
    "    y_test_sorted = y_test[sorted_indices]\n",
    "    mlp_preds_sorted = mlp_preds[sorted_indices]\n",
    "    kan_preds_sorted = kan_preds[sorted_indices]\n",
    "\n",
    "    # Plot Predictions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x_test_sorted, y_test_sorted, 'k-', label='True Function', linewidth=2)\n",
    "    plt.plot(x_test_sorted, mlp_preds_sorted, 'r--', label='MLP Predictions', linewidth=2)\n",
    "    plt.plot(x_test_sorted, kan_preds_sorted, 'b--', label='KAN Predictions', linewidth=2)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.title(f'Function Approximation for {func_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Residuals\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(x_test, mlp_preds - y_test, alpha=0.5, label='MLP Residuals')\n",
    "    plt.scatter(x_test, kan_preds - y_test, alpha=0.5, label='KAN Residuals')\n",
    "    plt.hlines(0, x_test.min(), x_test.max(), colors='k', linestyles='dashed')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.title(f'Residuals for {func_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Histograms of Residuals\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.hist(mlp_preds - y_test, bins=30, alpha=0.5, label='MLP Residuals')\n",
    "    plt.hist(kan_preds - y_test, bins=30, alpha=0.5, label='KAN Residuals')\n",
    "    plt.xlabel('Residual')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Residual Histogram for {func_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Creating Summary Table of Metrics\n",
    "summary_data = []\n",
    "\n",
    "for func_name in functions.keys():\n",
    "    mlp_metrics = results[func_name]['MLP']\n",
    "    kan_metrics = results[func_name]['KAN']\n",
    "\n",
    "    summary_data.append({\n",
    "        'Function': func_name,\n",
    "        'Model': 'MLP',\n",
    "        'MSE': mlp_metrics['test_loss'],\n",
    "        'MAE': mlp_metrics['mae'],\n",
    "        'RÂ²': mlp_metrics['r2'],\n",
    "        'MAPE (%)': mlp_metrics['mape'],\n",
    "        'Max Error': mlp_metrics['max_error']\n",
    "    })\n",
    "\n",
    "    summary_data.append({\n",
    "        'Function': func_name,\n",
    "        'Model': 'KAN',\n",
    "        'MSE': kan_metrics['test_loss'],\n",
    "        'MAE': kan_metrics['mae'],\n",
    "        'RÂ²': kan_metrics['r2'],\n",
    "        'MAPE (%)': kan_metrics['mape'],\n",
    "        'Max Error': kan_metrics['max_error']\n",
    "    })\n",
    "\n",
    "# Creating DataFrame\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Displaying the table\n",
    "print(\"\\nSummary of Metrics:\")\n",
    "print(summary_df)\n",
    "\n",
    "# Optionally, save the table to a CSV file\n",
    "summary_df.to_csv('model_comparison_summary.csv', index=False)\n",
    "\n",
    "# Visualizing Metrics with Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "metrics = ['MSE', 'MAE', 'RÂ²', 'MAPE (%)', 'Max Error']\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.barplot(x='Function', y=metric, hue='Model', data=summary_df)\n",
    "    plt.title(f'Comparison of {metric} between MLP and KAN')\n",
    "    plt.xlabel('Function')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend(title='Model')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envKAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
