{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Cell 1: Import Libraries and Set Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 2: Define Symbolic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple symbolic functions\n",
    "def symbolic_function_1(x):\n",
    "    return np.sin(5 * x) * np.exp(-x**2)\n",
    "\n",
    "def symbolic_function_2(x):\n",
    "    return np.where(x < 0, np.sin(5 * x), np.cos(5 * x)) * np.exp(-x**2)\n",
    "\n",
    "def symbolic_function_3(x):\n",
    "    return np.abs(np.sin(5 * x)) * np.exp(-x**2)\n",
    "\n",
    "def symbolic_function_4(x):\n",
    "    return np.sin(10 * x) * np.exp(-x**2)\n",
    "\n",
    "def symbolic_function_5(x):\n",
    "    return np.sin(15 * x) * np.exp(-x**2)\n",
    "\n",
    "def symbolic_function_6(x):\n",
    "    return np.piecewise(x, [x < 0, x >= 0], [lambda x: np.sin(5 * x), lambda x: np.sin(5 * x) + 1]) * np.exp(-x**2)\n",
    "\n",
    "# Dictionary of functions for easy access\n",
    "functions = {\n",
    "    'Function 1': symbolic_function_1,\n",
    "    'Function 2': symbolic_function_2,\n",
    "    'Function 3': symbolic_function_3,\n",
    "    'Function 4': symbolic_function_4,\n",
    "    'Function 5': symbolic_function_5,\n",
    "    'Function 6': symbolic_function_6,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3: Data Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(symbolic_function, n_samples=1000):\n",
    "    x_values = np.linspace(-2, 2, n_samples)\n",
    "    np.random.shuffle(x_values)\n",
    "    y_values = symbolic_function(x_values)\n",
    "\n",
    "    # Split into train, validation, and test sets\n",
    "    train_split = int(0.7 * n_samples)\n",
    "    val_split = int(0.85 * n_samples)\n",
    "\n",
    "    x_train = x_values[:train_split]\n",
    "    y_train = y_values[:train_split]\n",
    "\n",
    "    x_val = x_values[train_split:val_split]\n",
    "    y_val = y_values[train_split:val_split]\n",
    "\n",
    "    x_test = x_values[val_split:]\n",
    "    y_test = y_values[val_split:]\n",
    "\n",
    "    # Normalize inputs\n",
    "    x_mean = x_train.mean()\n",
    "    x_std = x_train.std()\n",
    "    x_train = (x_train - x_mean) / x_std\n",
    "    x_val = (x_val - x_mean) / x_std\n",
    "    x_test = (x_test - x_mean) / x_std\n",
    "\n",
    "    # Convert to tensors\n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    x_val_tensor = torch.tensor(x_val, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    x_test_tensor = torch.tensor(x_test, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, x_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 4: Define the MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Model Definition\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=20, output_dim=1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 5: Define the BSpline Activation and KAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BSpline Activation Function\n",
    "class BSplineActivation(nn.Module):\n",
    "    def __init__(self, num_parameters=20, init_range=(-1, 1)):\n",
    "        super(BSplineActivation, self).__init__()\n",
    "        # Initialize control points\n",
    "        self.control_points = nn.Parameter(torch.linspace(init_range[0], init_range[1], num_parameters))\n",
    "        # Initialize weights\n",
    "        self.weights = nn.Parameter(torch.randn(num_parameters))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the B-spline approximation using linear interpolation\n",
    "        x_clipped = torch.clamp(x, self.control_points[0].item(), self.control_points[-1].item())\n",
    "        indices = torch.bucketize(x_clipped.detach(), self.control_points.detach())\n",
    "        indices = torch.clamp(indices, 1, len(self.control_points) - 1)\n",
    "\n",
    "        x0 = self.control_points[indices - 1]\n",
    "        x1 = self.control_points[indices]\n",
    "        y0 = self.weights[indices - 1]\n",
    "        y1 = self.weights[indices]\n",
    "\n",
    "        slope = (y1 - y0) / (x1 - x0 + 1e-6)\n",
    "        output = y0 + slope * (x_clipped - x0)\n",
    "        return output\n",
    "\n",
    "# KAN Model Definition\n",
    "class KAN(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=20, output_dim=1, num_spline_points=20):\n",
    "        super(KAN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.act1 = BSplineActivation(num_parameters=num_spline_points)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.act2 = BSplineActivation(num_parameters=num_spline_points)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.act3 = BSplineActivation(num_parameters=num_spline_points)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 6: Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation Functions\n",
    "def train_model(model, optimizer, criterion, train_loader, val_loader, num_epochs=200, patience=20):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc='Training'):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "\n",
    "        scheduler.step(epoch_val_loss)\n",
    "\n",
    "        # Early Stopping\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Load the best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_model(model, criterion, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            preds.append(outputs.cpu().numpy())\n",
    "            trues.append(targets.cpu().numpy())\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "    preds = np.concatenate(preds).flatten()\n",
    "    trues = np.concatenate(trues).flatten()\n",
    "    return test_loss, preds, trues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 7: Main Loop to Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define criterion\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Store results for all functions\n",
    "results = {}\n",
    "\n",
    "for func_name, func in functions.items():\n",
    "    print(f\"\\nProcessing {func_name}...\")\n",
    "\n",
    "    # Generate data\n",
    "    train_loader, val_loader, test_loader, x_test, y_test = generate_data(func)\n",
    "\n",
    "    # Initialize models\n",
    "    mlp_model = MLP().to(device)\n",
    "    kan_model = KAN().to(device)\n",
    "\n",
    "    # Optimizers\n",
    "    mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=0.01)\n",
    "    kan_optimizer = optim.Adam(kan_model.parameters(), lr=0.01)\n",
    "\n",
    "    # Train MLP Model\n",
    "    print(\"Training MLP Model...\")\n",
    "    mlp_train_losses, mlp_val_losses = train_model(\n",
    "        mlp_model, mlp_optimizer, criterion, train_loader, val_loader\n",
    "    )\n",
    "    mlp_test_loss, mlp_preds, mlp_trues = evaluate_model(mlp_model, criterion, test_loader)\n",
    "    mlp_mae = mean_absolute_error(mlp_trues, mlp_preds)\n",
    "    mlp_r2 = r2_score(mlp_trues, mlp_preds)\n",
    "\n",
    "    # Train KAN Model\n",
    "    print(\"Training KAN Model...\")\n",
    "    kan_train_losses, kan_val_losses = train_model(\n",
    "        kan_model, kan_optimizer, criterion, train_loader, val_loader\n",
    "    )\n",
    "    kan_test_loss, kan_preds, kan_trues = evaluate_model(kan_model, criterion, test_loader)\n",
    "    kan_mae = mean_absolute_error(kan_trues, kan_preds)\n",
    "    kan_r2 = r2_score(kan_trues, kan_preds)\n",
    "\n",
    "    # Store results\n",
    "    results[func_name] = {\n",
    "        'MLP': {\n",
    "            'train_losses': mlp_train_losses,\n",
    "            'val_losses': mlp_val_losses,\n",
    "            'test_loss': mlp_test_loss,\n",
    "            'mae': mlp_mae,\n",
    "            'r2': mlp_r2,\n",
    "            'preds': mlp_preds,\n",
    "            'trues': mlp_trues,\n",
    "        },\n",
    "        'KAN': {\n",
    "            'train_losses': kan_train_losses,\n",
    "            'val_losses': kan_val_losses,\n",
    "            'test_loss': kan_test_loss,\n",
    "            'mae': kan_mae,\n",
    "            'r2': kan_r2,\n",
    "            'preds': kan_preds,\n",
    "            'trues': kan_trues,\n",
    "        },\n",
    "        'x_test': x_test,\n",
    "        'y_test': y_test,\n",
    "    }\n",
    "\n",
    "    # Plotting results\n",
    "    print(f\"\\nResults for {func_name}:\")\n",
    "\n",
    "    # Extract data\n",
    "    mlp_train_losses = results[func_name]['MLP']['train_losses']\n",
    "    mlp_val_losses = results[func_name]['MLP']['val_losses']\n",
    "    kan_train_losses = results[func_name]['KAN']['train_losses']\n",
    "    kan_val_losses = results[func_name]['KAN']['val_losses']\n",
    "\n",
    "    # Plot Training and Validation Losses\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(mlp_train_losses, label='MLP Training Loss')\n",
    "    plt.plot(mlp_val_losses, label='MLP Validation Loss')\n",
    "    plt.plot(kan_train_losses, label='KAN Training Loss')\n",
    "    plt.plot(kan_val_losses, label='KAN Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title(f'Training and Validation Losses for {func_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Test Metrics\n",
    "    print(f\"MLP Test MSE Loss: {results[func_name]['MLP']['test_loss']:.6f}\")\n",
    "    print(f\"MLP Test MAE: {results[func_name]['MLP']['mae']:.6f}\")\n",
    "    print(f\"MLP Test R^2 Score: {results[func_name]['MLP']['r2']:.6f}\")\n",
    "\n",
    "    print(f\"KAN Test MSE Loss: {results[func_name]['KAN']['test_loss']:.6f}\")\n",
    "    print(f\"KAN Test MAE: {results[func_name]['KAN']['mae']:.6f}\")\n",
    "    print(f\"KAN Test R^2 Score: {results[func_name]['KAN']['r2']:.6f}\")\n",
    "\n",
    "    # Plot Predictions vs True Function\n",
    "    x_test = results[func_name]['x_test']\n",
    "    y_test = results[func_name]['y_test']\n",
    "    mlp_preds = results[func_name]['MLP']['preds']\n",
    "    kan_preds = results[func_name]['KAN']['preds']\n",
    "\n",
    "    # Sort for plotting\n",
    "    sorted_indices = np.argsort(x_test)\n",
    "    x_test_sorted = x_test[sorted_indices]\n",
    "    y_test_sorted = y_test[sorted_indices]\n",
    "    mlp_preds_sorted = mlp_preds[sorted_indices]\n",
    "    kan_preds_sorted = kan_preds[sorted_indices]\n",
    "\n",
    "    # Plot Predictions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x_test_sorted, y_test_sorted, 'k-', label='True Function', linewidth=2)\n",
    "    plt.plot(x_test_sorted, mlp_preds_sorted, 'r--', label='MLP Predictions', linewidth=2)\n",
    "    plt.plot(x_test_sorted, kan_preds_sorted, 'b--', label='KAN Predictions', linewidth=2)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.title(f'Function Approximation for {func_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Residuals\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(x_test, mlp_preds - y_test, alpha=0.5, label='MLP Residuals')\n",
    "    plt.scatter(x_test, kan_preds - y_test, alpha=0.5, label='KAN Residuals')\n",
    "    plt.hlines(0, x_test.min(), x_test.max(), colors='k', linestyles='dashed')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.title(f'Residuals for {func_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Histograms of Residuals\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.hist(mlp_preds - y_test, bins=30, alpha=0.5, label='MLP Residuals')\n",
    "    plt.hist(kan_preds - y_test, bins=30, alpha=0.5, label='KAN Residuals')\n",
    "    plt.xlabel('Residual')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Residual Histogram for {func_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
