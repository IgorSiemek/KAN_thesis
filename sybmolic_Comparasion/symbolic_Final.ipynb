{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import all modules\n",
    "In this cell import all packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error,\n",
    "    max_error\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reproducibility and Device Setup\n",
    "\n",
    "Ensure that seed is constant and produces the same results and we select cpu/gpu for training. \n",
    "Seed applies for PyTorch and NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=4):\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility across Python, NumPy, and PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        seed (int): Seed value for all random number generators.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "# Select device: GPU if available, otherwise CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Mathematical Function Definitions\n",
    "\n",
    "In this Cell we define couple of methematical functions. Neural network will t ry to approximate. \n",
    "These functions ranging from symbolic (since, cosine, exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Symbolic Functions ---\n",
    "def symbolic_function_1(x):\n",
    "    \"\"\"\n",
    "    Computes f(x) = sin(5x) * exp(-x^2)\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): Input values.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Function values.\n",
    "    \"\"\"\n",
    "    return np.sin(5 * x) * np.exp(-x**2)\n",
    "\n",
    "def symbolic_function_2(x):\n",
    "    \"\"\"\n",
    "    Computes a conditional function:\n",
    "      - f(x) = sin(5x) if x < 0\n",
    "      - f(x) = cos(5x) if x >= 0\n",
    "    Multiplies the result by exp(-x^2).\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): Input values.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Function values.\n",
    "    \"\"\"\n",
    "    return np.where(x < 0, np.sin(5 * x), np.cos(5 * x)) * np.exp(-x**2)\n",
    "\n",
    "def symbolic_function_3(x):\n",
    "    \"\"\"\n",
    "    Computes f(x) = |sin(5x)| * exp(-x^2)\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): Input values.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Function values.\n",
    "    \"\"\"\n",
    "    return np.abs(np.sin(5 * x)) * np.exp(-x**2)\n",
    "\n",
    "def symbolic_function_4(x):\n",
    "    \"\"\"\n",
    "    Computes f(x) = sin(10x) * exp(-x^2)\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): Input values.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Function values.\n",
    "    \"\"\"\n",
    "    return np.sin(10 * x) * np.exp(-x**2)\n",
    "\n",
    "def symbolic_function_5(x):\n",
    "    \"\"\"\n",
    "    Computes f(x) = sin(15x) * exp(-x^2)\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): Input values.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Function values.\n",
    "    \"\"\"\n",
    "    return np.sin(15 * x) * np.exp(-x**2)\n",
    "\n",
    "def symbolic_function_6(x):\n",
    "    \"\"\"\n",
    "    Computes a piecewise function:\n",
    "      - f(x) = sin(5x) for x < 0\n",
    "      - f(x) = sin(5x) + 1 for x >= 0\n",
    "    Then multiplies the result by exp(-x^2).\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): Input values.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Function values.\n",
    "    \"\"\"\n",
    "    return np.piecewise(x, [x < 0, x >= 0], \n",
    "                          [lambda x: np.sin(5 * x), lambda x: np.sin(5 * x) + 1]) * np.exp(-x**2)\n",
    "\n",
    "# --- Additional Functions: Polynomials, Exponentials, Logarithms, etc. ---\n",
    "def polynomial_function_1(x):\n",
    "    \"\"\"Computes f(x) = x^2.\"\"\"\n",
    "    return x**2\n",
    "\n",
    "def polynomial_function_2(x):\n",
    "    \"\"\"Computes f(x) = x^3 - 2x + 1.\"\"\"\n",
    "    return x**3 - 2*x + 1\n",
    "\n",
    "def exponential_function_1(x):\n",
    "    \"\"\"Computes f(x) = exp(x).\"\"\"\n",
    "    return np.exp(x)\n",
    "\n",
    "def exponential_function_2(x):\n",
    "    \"\"\"Computes f(x) = 2 * exp(0.5 * x).\"\"\"\n",
    "    return 2 * np.exp(0.5 * x)\n",
    "\n",
    "def logarithmic_function_1(x):\n",
    "    \"\"\"Computes f(x) = log(|x| + 1).\"\"\"\n",
    "    return np.log(np.abs(x) + 1)\n",
    "\n",
    "def logarithmic_function_2(x):\n",
    "    \"\"\"Computes f(x) = log10(|x| + 1).\"\"\"\n",
    "    return np.log10(np.abs(x) + 1)\n",
    "\n",
    "def trigonometric_function_3(x):\n",
    "    \"\"\"Computes f(x) = tan(x) * exp(-x^2).\"\"\"\n",
    "    return np.tan(x) * np.exp(-x**2)\n",
    "\n",
    "def trigonometric_function_4(x):\n",
    "    \"\"\"Computes f(x) = (1/tan(x)) * exp(-x^2).\"\"\"\n",
    "    return (1 / np.tan(x)) * np.exp(-x**2)\n",
    "\n",
    "def step_function_1(x):\n",
    "    \"\"\"Computes a step function: f(x) = 0 if x < 0, else 1.\"\"\"\n",
    "    return np.where(x < 0, 0, 1)\n",
    "\n",
    "def step_function_2(x):\n",
    "    \"\"\"Computes a step function: f(x) = 0 if x < -1, else 1.\"\"\"\n",
    "    return np.where(x < -1, 0, 1)\n",
    "\n",
    "def uniform_function_1(x):\n",
    "    \"\"\"Returns a constant function f(x) = 5.\"\"\"\n",
    "    return 5 * np.ones_like(x)\n",
    "\n",
    "def uniform_function_2(x):\n",
    "    \"\"\"\n",
    "    Computes f(x) = 3x + 2 with added uniform noise in the range [-0.5, 0.5].\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): Input values.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Function values with noise.\n",
    "    \"\"\"\n",
    "    return 3 * x + 2 + np.random.uniform(-0.5, 0.5, size=x.shape)\n",
    "\n",
    "# --- Dictionary for Easy Function Access ---\n",
    "functions = {\n",
    "    'Function 1': symbolic_function_1,\n",
    "    'Function 2': symbolic_function_2,\n",
    "    'Function 3': symbolic_function_3,\n",
    "    'Function 4': symbolic_function_4,\n",
    "    'Function 5': symbolic_function_5,\n",
    "    'Function 6': symbolic_function_6,\n",
    "    'Polynomial 1': polynomial_function_1,\n",
    "    'Polynomial 2': polynomial_function_2,\n",
    "    'Exponential 1': exponential_function_1,\n",
    "    'Exponential 2': exponential_function_2,\n",
    "    'Logarithmic 1': logarithmic_function_1,\n",
    "    'Logarithmic 2': logarithmic_function_2,\n",
    "    'Trigonometric 3': trigonometric_function_3,\n",
    "    'Trigonometric 4': trigonometric_function_4,\n",
    "    'Step 1': step_function_1,\n",
    "    'Step 2': step_function_2,\n",
    "    'Uniform 1': uniform_function_1,\n",
    "    'Uniform 2': uniform_function_2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Generation and Normalization\n",
    "\n",
    "Generate input-output pairs from a given mathematical function, split the data into training/validation/testing sets and normalize them.\n",
    "Tested, 1000 samples is enough for representing functions.expl\n",
    "Range of x is -2 to 2\n",
    "Order is shuffled so that the train, validation and test sets are randomized\n",
    "Training 70% data, Validation 15%, Testing 15% sets are created\n",
    "Than we normalize it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(symbolic_function, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generates data samples using the provided function, splits the data into\n",
    "    training, validation, and testing sets, and applies normalization to both\n",
    "    inputs and outputs.\n",
    "    \n",
    "    Args:\n",
    "        symbolic_function (callable): The function to generate y-values.\n",
    "        n_samples (int): Total number of samples to generate.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader, test_loader, x_test (numpy array),\n",
    "                y_test_orig (original-scale test targets), y_scaler)\n",
    "    \"\"\"\n",
    "    # Create evenly spaced x-values and shuffle them\n",
    "    x_values = np.linspace(-2, 2, n_samples)\n",
    "    np.random.shuffle(x_values)\n",
    "    y_values = symbolic_function(x_values)\n",
    "\n",
    "    # Split data: 70% training, 15% validation, 15% test\n",
    "    train_split = int(0.7 * n_samples)\n",
    "    val_split = int(0.85 * n_samples)\n",
    "\n",
    "    x_train = x_values[:train_split]\n",
    "    y_train = y_values[:train_split]\n",
    "    x_val = x_values[train_split:val_split]\n",
    "    y_val = y_values[train_split:val_split]\n",
    "    x_test = x_values[val_split:]\n",
    "    y_test = y_values[val_split:]\n",
    "\n",
    "    # Normalize inputs using training mean and std\n",
    "    x_mean = x_train.mean()\n",
    "    x_std = x_train.std()\n",
    "    x_train = (x_train - x_mean) / x_std\n",
    "    x_val = (x_val - x_mean) / x_std\n",
    "    x_test = (x_test - x_mean) / x_std\n",
    "\n",
    "    # Normalize outputs with MinMaxScaler based on training data\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_val_scaled = y_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
    "    y_test_scaled = y_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Obtain the inverse-transformed test targets for plotting\n",
    "    y_test_orig = y_scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors and add a feature dimension\n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    x_val_tensor = torch.tensor(x_val, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    x_test_tensor = torch.tensor(x_test, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    # Create DataLoaders for batching during training and evaluation\n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, x_test, y_test_orig, y_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Model Definitions\n",
    "a) MLP\n",
    "Simple feedforward neural network for function approximation\n",
    "It has Input Layers and thee hidden layers (each is using Tanh activation), and final output layers\n",
    "    Forward: method passes the input through these layeres sequentially\n",
    "\n",
    "b) BSpline Activation\n",
    "Here we create our own BSpline activation function.\n",
    "It's idea is to have learnable function.\n",
    "Module create set of control points (evenly spaced within a given range) and correspoing weights (initalized randomly)\n",
    "    Forward for each input value:\n",
    "        - Check if argument is beetwen smallest and largest point if not it assign it's value to it. If yes it preserve it's value\n",
    "        - Bucketize so determine in which segment input value lies \n",
    "        - Preform linear interpolation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MLP Model ---\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A Multi-Layer Perceptron for function approximation.\n",
    "    \n",
    "    Architecture:\n",
    "      - Input layer\n",
    "      - Three hidden layers with Tanh activation functions\n",
    "      - Output layer\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): Number of input features.\n",
    "        hidden_dim (int): Number of neurons in each hidden layer.\n",
    "        output_dim (int): Number of output features.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=1, hidden_dim=14, output_dim=1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the MLP.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output predictions.\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "# --- BSpline Activation for KAN Model ---\n",
    "class BSplineActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom B-Spline Activation function with learnable control points and weights.\n",
    "    \n",
    "    This activation function performs a piecewise linear interpolation based\n",
    "    on learnable parameters.\n",
    "    \n",
    "    Args:\n",
    "        num_parameters (int): Number of control points.\n",
    "        init_range (tuple): Range for initializing control points.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_parameters=20, init_range=(-2, 2)):\n",
    "        super(BSplineActivation, self).__init__()\n",
    "        # Initialize control points evenly spaced in the given range.\n",
    "        self.control_points = nn.Parameter(torch.linspace(init_range[0], init_range[1], num_parameters))\n",
    "        # Initialize weights with small random values.\n",
    "        self.weights = nn.Parameter(torch.randn(num_parameters) * 0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Applies the B-Spline activation.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Activated tensor.\n",
    "        \"\"\"\n",
    "        # Ensure inputs are within the control points' range.\n",
    "        x_clipped = torch.clamp(x, self.control_points[0], self.control_points[-1])\n",
    "        # Find the corresponding bucket indices for each x.\n",
    "        indices = torch.bucketize(x_clipped, self.control_points)\n",
    "        indices = torch.clamp(indices, 1, len(self.control_points) - 1)\n",
    "        x0 = self.control_points[indices - 1]\n",
    "        x1 = self.control_points[indices]\n",
    "        y0 = self.weights[indices - 1]\n",
    "        y1 = self.weights[indices]\n",
    "        # Linear interpolation between y0 and y1.\n",
    "        slope = (y1 - y0) / (x1 - x0 + 1e-6)\n",
    "        output = y0 + slope * (x_clipped - x0)\n",
    "        return output\n",
    "\n",
    "# --- KAN Model ---\n",
    "class KAN(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network model (KAN) that uses custom B-Spline activations.\n",
    "    \n",
    "    Architecture:\n",
    "      - Three fully-connected layers each followed by a BSplineActivation\n",
    "        and dropout for regularization.\n",
    "      - Final output layer.\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): Number of input features.\n",
    "        hidden_dim (int): Number of neurons in hidden layers.\n",
    "        output_dim (int): Number of output features.\n",
    "        num_spline_points (int): Number of spline control points per activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=1, hidden_dim=10, output_dim=1, num_spline_points=30):\n",
    "        super(KAN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.act1 = BSplineActivation(num_parameters=num_spline_points)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.act2 = BSplineActivation(num_parameters=num_spline_points)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.act3 = BSplineActivation(num_parameters=num_spline_points)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the KAN model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output predictions.\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training and Evaluation Functions\n",
    "\n",
    "Train a given model using the generated training data while monitoring it's validation loss, adjusting learing rate and implemennting early stopping.\n",
    "\n",
    "Training Loop:\n",
    "    Model is set to training mode\n",
    "    Code iterates over batches from training DataLoder, computes predictions, calculate the loss (using MSE) and update model parameters\n",
    "\n",
    "    Validation:\n",
    "        After each epoch, the model is evaluated on the validation set to compute a validation loss.\n",
    "        Use ReduceLROnPlateau that redcues learning rate if the validation loss does not improve\n",
    "        If validation loss does not improve for specified patience we stop learning early\n",
    "    Evaluation:\n",
    "        Function sets the model to evaluate mode and iterates through the test data, computing loss and storing predictions\n",
    "        Predictions and true outputs (which we scaled during preprocessing) are converted back to orginal scale using scaler\n",
    "        Metrics Calculation:\n",
    "            MAE\n",
    "            R*2 Score\n",
    "            MAPE\n",
    "            ME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, train_loader, val_loader, num_epochs=100, patience=50):\n",
    "    \"\"\"\n",
    "    Train the provided model using the specified optimizer and loss function.\n",
    "    \n",
    "    Implements early stopping based on validation loss and includes learning\n",
    "    rate scheduling.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Neural network model.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "        num_epochs (int): Maximum number of training epochs.\n",
    "        patience (int): Number of epochs to wait for improvement before stopping.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Lists of training and validation losses per epoch.\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Reduce learning rate if the validation loss plateaus.\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc='Training'):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            # Clip gradients to prevent exploding gradients.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "\n",
    "        scheduler.step(epoch_val_loss)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Restore the best model weights.\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_model(model, criterion, test_loader, y_scaler):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model on test data and compute error metrics.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        test_loader (DataLoader): DataLoader for test data.\n",
    "        y_scaler (MinMaxScaler): Scaler for inverse-transforming outputs.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (test_loss, MAE, R², MAPE, Max Error, predictions in original scale,\n",
    "                true targets in original scale)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            preds.append(outputs.cpu().numpy())\n",
    "            trues.append(targets.cpu().numpy())\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "    preds = np.concatenate(preds).flatten()\n",
    "    trues = np.concatenate(trues).flatten()\n",
    "\n",
    "    # Inverse transform predictions and true targets to original scale.\n",
    "    preds_orig = y_scaler.inverse_transform(preds.reshape(-1, 1)).flatten()\n",
    "    trues_orig = y_scaler.inverse_transform(trues.reshape(-1, 1)).flatten()\n",
    "\n",
    "    mae = mean_absolute_error(trues_orig, preds_orig)\n",
    "    r2 = r2_score(trues_orig, preds_orig)\n",
    "    mape = mean_absolute_percentage_error(trues_orig, preds_orig)\n",
    "    max_err = max_error(trues_orig, preds_orig)\n",
    "\n",
    "    return test_loss, mae, r2, mape, max_err, preds_orig, trues_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Main Training Loop\n",
    "\n",
    "Use all previously created classes and functions and train both MLP and KAN models on the data generated from each function, evalute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "results = {}\n",
    "\n",
    "# Loop over all defined functions for comparison.\n",
    "print(\"MLP model parameter count:\", sum(p.numel() for p in mlp_model.parameters()))\n",
    "print(\"KAN model parameter count:\", sum(p.numel() for p in kan_model.parameters()))\n",
    "for func_name, func in functions.items():\n",
    "    print(f\"\\nProcessing {func_name}...\")\n",
    "\n",
    "    # Generate normalized data from the current function.\n",
    "    train_loader, val_loader, test_loader, x_test, y_test_orig, y_scaler = generate_data(func)\n",
    "\n",
    "    # Initialize models and print parameter counts.\n",
    "    mlp_model = MLP().to(device)\n",
    "    kan_model = KAN().to(device)\n",
    "\n",
    "    # Set up optimizers.\n",
    "    mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "    kan_optimizer = optim.Adam(kan_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "    # Train and evaluate the MLP model.\n",
    "    print(\"Training MLP Model...\")\n",
    "    mlp_train_losses, mlp_val_losses = train_model(mlp_model, mlp_optimizer, criterion, train_loader, val_loader)\n",
    "    mlp_test_loss, mlp_mae, mlp_r2, mlp_mape, mlp_max_err, mlp_preds, mlp_trues = evaluate_model(\n",
    "        mlp_model, criterion, test_loader, y_scaler\n",
    "    )\n",
    "\n",
    "    # Train and evaluate the KAN model.\n",
    "    print(\"Training KAN Model...\")\n",
    "    kan_train_losses, kan_val_losses = train_model(kan_model, kan_optimizer, criterion, train_loader, val_loader)\n",
    "    kan_test_loss, kan_mae, kan_r2, kan_mape, kan_max_err, kan_preds, kan_trues = evaluate_model(\n",
    "        kan_model, criterion, test_loader, y_scaler\n",
    "    )\n",
    "\n",
    "    # Store all metrics and predictions for later analysis.\n",
    "    results[func_name] = {\n",
    "        'MLP': {\n",
    "            'train_losses': mlp_train_losses,\n",
    "            'val_losses': mlp_val_losses,\n",
    "            'test_loss': mlp_test_loss,\n",
    "            'mae': mlp_mae,\n",
    "            'r2': mlp_r2,\n",
    "            'mape': mlp_mape,\n",
    "            'max_error': mlp_max_err,\n",
    "            'preds': mlp_preds,\n",
    "            'trues': mlp_trues,\n",
    "        },\n",
    "        'KAN': {\n",
    "            'train_losses': kan_train_losses,\n",
    "            'val_losses': kan_val_losses,\n",
    "            'test_loss': kan_test_loss,\n",
    "            'mae': kan_mae,\n",
    "            'r2': kan_r2,\n",
    "            'mape': kan_mape,\n",
    "            'max_error': kan_max_err,\n",
    "            'preds': kan_preds,\n",
    "            'trues': kan_trues,\n",
    "        },\n",
    "        'x_test': x_test,\n",
    "        'y_test': y_test_orig,\n",
    "    }\n",
    "\n",
    "    # ---------------------------\n",
    "    # Plot Training and Validation Losses\n",
    "    # ---------------------------\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(mlp_train_losses, label='MLP Training Loss')\n",
    "    plt.plot(mlp_val_losses, label='MLP Validation Loss')\n",
    "    plt.plot(kan_train_losses, label='KAN Training Loss')\n",
    "    plt.plot(kan_val_losses, label='KAN Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title(f'Training and Validation Losses for {func_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # ---------------------------\n",
    "    # Print Test Metrics for Both Models\n",
    "    # ---------------------------\n",
    "    print(f\"MLP Test MSE Loss: {results[func_name]['MLP']['test_loss']:.6f}\")\n",
    "    print(f\"MLP Test MAE: {results[func_name]['MLP']['mae']:.6f}\")\n",
    "    print(f\"MLP Test R^2 Score: {results[func_name]['MLP']['r2']:.6f}\")\n",
    "    print(f\"MLP Test MAPE: {results[func_name]['MLP']['mape']:.2f}%\")\n",
    "    print(f\"MLP Test Max Error: {results[func_name]['MLP']['max_error']:.6f}\")\n",
    "    \n",
    "    print(f\"KAN Test MSE Loss: {results[func_name]['KAN']['test_loss']:.6f}\")\n",
    "    print(f\"KAN Test MAE: {results[func_name]['KAN']['mae']:.6f}\")\n",
    "    print(f\"KAN Test R^2 Score: {results[func_name]['KAN']['r2']:.6f}\")\n",
    "    print(f\"KAN Test MAPE: {results[func_name]['KAN']['mape']:.2f}%\")\n",
    "    print(f\"KAN Test Max Error: {results[func_name]['KAN']['max_error']:.6f}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Plot Predictions vs. True Function\n",
    "    # ---------------------------\n",
    "    x_test = results[func_name]['x_test']\n",
    "    mlp_preds = results[func_name]['MLP']['preds']\n",
    "    kan_preds = results[func_name]['KAN']['preds']\n",
    "    true_vals = results[func_name]['MLP']['trues']  # True targets are the same for both models.\n",
    "\n",
    "    # Sort the test data for a cleaner plot.\n",
    "    sorted_indices = np.argsort(x_test)\n",
    "    x_test_sorted = x_test[sorted_indices]\n",
    "    true_sorted = true_vals[sorted_indices]\n",
    "    mlp_preds_sorted = mlp_preds[sorted_indices]\n",
    "    kan_preds_sorted = kan_preds[sorted_indices]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x_test_sorted, true_sorted, 'k-', label='True Function', linewidth=2)\n",
    "    plt.plot(x_test_sorted, mlp_preds_sorted, 'r--', label='MLP Predictions', linewidth=2)\n",
    "    plt.plot(x_test_sorted, kan_preds_sorted, 'b--', label='KAN Predictions', linewidth=2)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.title(f'Function Approximation for {func_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # ---------------------------\n",
    "    # Plot Residuals (Predicted - True)\n",
    "    # ---------------------------\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(x_test, mlp_preds - true_vals, alpha=0.5, label='MLP Residuals')\n",
    "    plt.scatter(x_test, kan_preds - true_vals, alpha=0.5, label='KAN Residuals')\n",
    "    plt.hlines(0, x_test.min(), x_test.max(), colors='k', linestyles='dashed')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.title(f'Residuals for {func_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # ---------------------------\n",
    "    # Plot Histogram of Residuals\n",
    "    # ---------------------------\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(mlp_preds - true_vals, bins=30, alpha=0.5, label='MLP Residuals')\n",
    "    plt.hist(kan_preds - true_vals, bins=30, alpha=0.5, label='KAN Residuals')\n",
    "    plt.xlabel('Residual')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Residual Histogram for {func_name}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Showing result of experiments\n",
    "\n",
    "Mean Square Error -> Average of square errors.\n",
    "    It is senstivie to outliners (big errors)\n",
    "    It is often used as a loss function during training\n",
    "\n",
    "Mean Abolute Error -> Avearge of absolute errors\n",
    "    Less sensitive to outliners\n",
    "    Eaiser to see diff\n",
    "    \n",
    "R^2 -> How well the model predictions approximate actual data\n",
    "\n",
    "MAPE -> Mean Absolute Peratance Erros\n",
    "    It measures avearge absoulte perecnt error\n",
    "\n",
    "MAX Error -> Biggest error, outliner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = []\n",
    "\n",
    "# Create a summary table that collects error metrics for both models across functions.\n",
    "for func_name in functions.keys():\n",
    "    mlp_metrics = results[func_name]['MLP']\n",
    "    kan_metrics = results[func_name]['KAN']\n",
    "\n",
    "    summary_data.append({\n",
    "        'Function': func_name,\n",
    "        'Model': 'MLP',\n",
    "        'MSE': mlp_metrics['test_loss'],\n",
    "        'MAE': mlp_metrics['mae'],\n",
    "        'R²': mlp_metrics['r2'],\n",
    "        'MAPE (%)': mlp_metrics['mape'],\n",
    "        'Max Error': mlp_metrics['max_error']\n",
    "    })\n",
    "\n",
    "    summary_data.append({\n",
    "        'Function': func_name,\n",
    "        'Model': 'KAN',\n",
    "        'MSE': kan_metrics['test_loss'],\n",
    "        'MAE': kan_metrics['mae'],\n",
    "        'R²': kan_metrics['r2'],\n",
    "        'MAPE (%)': kan_metrics['mape'],\n",
    "        'Max Error': kan_metrics['max_error']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nSummary of Metrics:\")\n",
    "print(summary_df)\n",
    "summary_df.to_csv('model_comparison_summary.csv', index=False)\n",
    "\n",
    "# Visualize the metrics for comparison using Seaborn bar plots.\n",
    "sns.set(style=\"whitegrid\")\n",
    "metrics = ['MSE', 'MAE', 'R²', 'MAPE (%)', 'Max Error']\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.barplot(x='Function', y=metric, hue='Model', data=summary_df)\n",
    "    plt.title(f'Comparison of {metric} between MLP and KAN')\n",
    "    plt.xlabel('Function')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend(title='Model')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envKAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
