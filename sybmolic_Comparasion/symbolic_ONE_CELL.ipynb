{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 1. Reproducibility and Device Setup\n",
    "# =============================================================================\n",
    "\n",
    "def seed_everything(seed=4):\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility across Python, NumPy, and PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        seed (int): Seed value for all random number generators.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "# Select device: GPU if available, otherwise CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Mathematical Function Definitions\n",
    "# =============================================================================\n",
    "\n",
    "# --- Symbolic Functions ---\n",
    "def symbolic_function_1(x):\n",
    "    \"\"\"\n",
    "    Computes f(x) = sin(5x) * exp(-x^2)\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): Input values.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Function values.\n",
    "    \"\"\"\n",
    "    return np.sin(5 * x) * np.exp(-x**2)\n",
    "\n",
    "def symbolic_function_2(x):\n",
    "    \"\"\"\n",
    "    Computes a conditional function:\n",
    "      - f(x) = sin(5x) if x < 0\n",
    "      - f(x) = cos(5x) if x >= 0\n",
    "    Multiplies the result by exp(-x^2).\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): Input values.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Function values.\n",
    "    \"\"\"\n",
    "    return np.where(x < 0, np.sin(5 * x), np.cos(5 * x)) * np.exp(-x**2)\n",
    "\n",
    "def symbolic_function_3(x):\n",
    "    \"\"\"\n",
    "    Computes f(x) = |sin(5x)| * exp(-x^2)\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): Input values.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Function values.\n",
    "    \"\"\"\n",
    "    return np.abs(np.sin(5 * x)) * np.exp(-x**2)\n",
    "\n",
    "def symbolic_function_4(x):\n",
    "    \"\"\"\n",
    "    Computes f(x) = sin(10x) * exp(-x^2)\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): Input values.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Function values.\n",
    "    \"\"\"\n",
    "    return np.sin(10 * x) * np.exp(-x**2)\n",
    "\n",
    "def symbolic_function_5(x):\n",
    "    \"\"\"\n",
    "    Computes f(x) = sin(15x) * exp(-x^2)\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): Input values.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Function values.\n",
    "    \"\"\"\n",
    "    return np.sin(15 * x) * np.exp(-x**2)\n",
    "\n",
    "def symbolic_function_6(x):\n",
    "    \"\"\"\n",
    "    Computes a piecewise function:\n",
    "      - f(x) = sin(5x) for x < 0\n",
    "      - f(x) = sin(5x) + 1 for x >= 0\n",
    "    Then multiplies the result by exp(-x^2).\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): Input values.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Function values.\n",
    "    \"\"\"\n",
    "    return np.piecewise(x, [x < 0, x >= 0], \n",
    "                          [lambda x: np.sin(5 * x), lambda x: np.sin(5 * x) + 1]) * np.exp(-x**2)\n",
    "\n",
    "# --- Additional Functions: Polynomials, Exponentials, Logarithms, etc. ---\n",
    "def polynomial_function_1(x):\n",
    "    \"\"\"Computes f(x) = x^2.\"\"\"\n",
    "    return x**2\n",
    "\n",
    "def polynomial_function_2(x):\n",
    "    \"\"\"Computes f(x) = x^3 - 2x + 1.\"\"\"\n",
    "    return x**3 - 2*x + 1\n",
    "\n",
    "def exponential_function_1(x):\n",
    "    \"\"\"Computes f(x) = exp(x).\"\"\"\n",
    "    return np.exp(x)\n",
    "\n",
    "def exponential_function_2(x):\n",
    "    \"\"\"Computes f(x) = 2 * exp(0.5 * x).\"\"\"\n",
    "    return 2 * np.exp(0.5 * x)\n",
    "\n",
    "def logarithmic_function_1(x):\n",
    "    \"\"\"Computes f(x) = log(|x| + 1).\"\"\"\n",
    "    return np.log(np.abs(x) + 1)\n",
    "\n",
    "def logarithmic_function_2(x):\n",
    "    \"\"\"Computes f(x) = log10(|x| + 1).\"\"\"\n",
    "    return np.log10(np.abs(x) + 1)\n",
    "\n",
    "def trigonometric_function_3(x):\n",
    "    \"\"\"Computes f(x) = tan(x) * exp(-x^2).\"\"\"\n",
    "    return np.tan(x) * np.exp(-x**2)\n",
    "\n",
    "def trigonometric_function_4(x):\n",
    "    \"\"\"Computes f(x) = (1/tan(x)) * exp(-x^2).\"\"\"\n",
    "    return (1 / np.tan(x)) * np.exp(-x**2)\n",
    "\n",
    "def step_function_1(x):\n",
    "    \"\"\"Computes a step function: f(x) = 0 if x < 0, else 1.\"\"\"\n",
    "    return np.where(x < 0, 0, 1)\n",
    "\n",
    "def step_function_2(x):\n",
    "    \"\"\"Computes a step function: f(x) = 0 if x < -1, else 1.\"\"\"\n",
    "    return np.where(x < -1, 0, 1)\n",
    "\n",
    "def uniform_function_1(x):\n",
    "    \"\"\"Returns a constant function f(x) = 5.\"\"\"\n",
    "    return 5 * np.ones_like(x)\n",
    "\n",
    "def uniform_function_2(x):\n",
    "    \"\"\"\n",
    "    Computes f(x) = 3x + 2 with added uniform noise in the range [-0.5, 0.5].\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): Input values.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Function values with noise.\n",
    "    \"\"\"\n",
    "    return 3 * x + 2 + np.random.uniform(-0.5, 0.5, size=x.shape)\n",
    "\n",
    "# --- Dictionary for Easy Function Access ---\n",
    "functions = {\n",
    "    'Function 1': symbolic_function_1,\n",
    "    'Function 2': symbolic_function_2,\n",
    "    'Function 3': symbolic_function_3,\n",
    "    'Function 4': symbolic_function_4,\n",
    "    'Function 5': symbolic_function_5,\n",
    "    'Function 6': symbolic_function_6,\n",
    "    'Polynomial 1': polynomial_function_1,\n",
    "    'Polynomial 2': polynomial_function_2,\n",
    "    'Exponential 1': exponential_function_1,\n",
    "    'Exponential 2': exponential_function_2,\n",
    "    'Logarithmic 1': logarithmic_function_1,\n",
    "    'Logarithmic 2': logarithmic_function_2,\n",
    "    'Trigonometric 3': trigonometric_function_3,\n",
    "    'Trigonometric 4': trigonometric_function_4,\n",
    "    'Step 1': step_function_1,\n",
    "    'Step 2': step_function_2,\n",
    "    'Uniform 1': uniform_function_1,\n",
    "    'Uniform 2': uniform_function_2,\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Data Generation and Normalization\n",
    "# =============================================================================\n",
    "\n",
    "def generate_data(symbolic_function, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generates data samples using the provided function, splits the data into\n",
    "    training, validation, and testing sets, and applies normalization to both\n",
    "    inputs and outputs.\n",
    "    \n",
    "    Args:\n",
    "        symbolic_function (callable): The function to generate y-values.\n",
    "        n_samples (int): Total number of samples to generate.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader, test_loader, x_test (numpy array),\n",
    "                y_test_orig (original-scale test targets), y_scaler)\n",
    "    \"\"\"\n",
    "    # Create evenly spaced x-values and shuffle them\n",
    "    x_values = np.linspace(-2, 2, n_samples)\n",
    "    np.random.shuffle(x_values)\n",
    "    y_values = symbolic_function(x_values)\n",
    "\n",
    "    # Split data: 70% training, 15% validation, 15% test\n",
    "    train_split = int(0.7 * n_samples)\n",
    "    val_split = int(0.85 * n_samples)\n",
    "\n",
    "    x_train = x_values[:train_split]\n",
    "    y_train = y_values[:train_split]\n",
    "    x_val = x_values[train_split:val_split]\n",
    "    y_val = y_values[train_split:val_split]\n",
    "    x_test = x_values[val_split:]\n",
    "    y_test = y_values[val_split:]\n",
    "\n",
    "    # Normalize inputs using training mean and std\n",
    "    x_mean = x_train.mean()\n",
    "    x_std = x_train.std()\n",
    "    x_train = (x_train - x_mean) / x_std\n",
    "    x_val = (x_val - x_mean) / x_std\n",
    "    x_test = (x_test - x_mean) / x_std\n",
    "\n",
    "    # Normalize outputs with MinMaxScaler based on training data\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_val_scaled = y_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
    "    y_test_scaled = y_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Obtain the inverse-transformed test targets for plotting\n",
    "    y_test_orig = y_scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors and add a feature dimension\n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    x_val_tensor = torch.tensor(x_val, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    x_test_tensor = torch.tensor(x_test, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    # Create DataLoaders for batching during training and evaluation\n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, x_test, y_test_orig, y_scaler\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Training and Evaluation Functions\n",
    "# =============================================================================\n",
    "\n",
    "def train_model(model, optimizer, criterion, train_loader, val_loader, num_epochs=100, patience=50):\n",
    "    \"\"\"\n",
    "    Train the provided model using the specified optimizer and loss function.\n",
    "    \n",
    "    Implements early stopping based on validation loss and includes learning\n",
    "    rate scheduling.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Neural network model.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "        num_epochs (int): Maximum number of training epochs.\n",
    "        patience (int): Number of epochs to wait for improvement before stopping.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Lists of training and validation losses per epoch.\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Reduce learning rate if the validation loss plateaus.\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc='Training'):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            # Clip gradients to prevent exploding gradients.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "\n",
    "        scheduler.step(epoch_val_loss)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Restore the best model weights.\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_model(model, criterion, test_loader, y_scaler):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model on test data and compute error metrics.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        test_loader (DataLoader): DataLoader for test data.\n",
    "        y_scaler (MinMaxScaler): Scaler for inverse-transforming outputs.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (test_loss, MAE, R², MAPE, Max Error, predictions in original scale,\n",
    "                true targets in original scale)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            preds.append(outputs.cpu().numpy())\n",
    "            trues.append(targets.cpu().numpy())\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "    preds = np.concatenate(preds).flatten()\n",
    "    trues = np.concatenate(trues).flatten()\n",
    "\n",
    "    # Inverse transform predictions and true targets to original scale.\n",
    "    preds_orig = y_scaler.inverse_transform(preds.reshape(-1, 1)).flatten()\n",
    "    trues_orig = y_scaler.inverse_transform(trues.reshape(-1, 1)).flatten()\n",
    "\n",
    "    mae = mean_absolute_error(trues_orig, preds_orig)\n",
    "    r2 = r2_score(trues_orig, preds_orig)\n",
    "    mape = mean_absolute_percentage_error(trues_orig, preds_orig)\n",
    "    max_err = max_error(trues_orig, preds_orig)\n",
    "\n",
    "    return test_loss, mae, r2, mape, max_err, preds_orig, trues_orig\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Main Training Loop for Model Comparison\n",
    "# =============================================================================\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "results = {}\n",
    "\n",
    "# Loop over all defined functions for comparison.\n",
    "for func_name, func in functions.items():\n",
    "    print(f\"\\nProcessing {func_name}...\")\n",
    "\n",
    "    # Generate normalized data from the current function.\n",
    "    train_loader, val_loader, test_loader, x_test, y_test_orig, y_scaler = generate_data(func)\n",
    "\n",
    "    # Initialize models and print parameter counts.\n",
    "    mlp_model = MLP().to(device)\n",
    "    kan_model = KAN().to(device)\n",
    "    print(\"MLP model parameter count:\", sum(p.numel() for p in mlp_model.parameters()))\n",
    "    print(\"KAN model parameter count:\", sum(p.numel() for p in kan_model.parameters()))\n",
    "\n",
    "    # Set up optimizers.\n",
    "    mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "    kan_optimizer = optim.Adam(kan_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "    # Train and evaluate the MLP model.\n",
    "    print(\"Training MLP Model...\")\n",
    "    mlp_train_losses, mlp_val_losses = train_model(mlp_model, mlp_optimizer, criterion, train_loader, val_loader)\n",
    "    mlp_test_loss, mlp_mae, mlp_r2, mlp_mape, mlp_max_err, mlp_preds, mlp_trues = evaluate_model(\n",
    "        mlp_model, criterion, test_loader, y_scaler\n",
    "    )\n",
    "\n",
    "    # Train and evaluate the KAN model.\n",
    "    print(\"Training KAN Model...\")\n",
    "    kan_train_losses, kan_val_losses = train_model(kan_model, kan_optimizer, criterion, train_loader, val_loader)\n",
    "    kan_test_loss, kan_mae, kan_r2, kan_mape, kan_max_err, kan_preds, kan_trues = evaluate_model(\n",
    "        kan_model, criterion, test_loader, y_scaler\n",
    "    )\n",
    "\n",
    "    # Store all metrics and predictions for later analysis.\n",
    "    results[func_name] = {\n",
    "        'MLP': {\n",
    "            'train_losses': mlp_train_losses,\n",
    "            'val_losses': mlp_val_losses,\n",
    "            'test_loss': mlp_test_loss,\n",
    "            'mae': mlp_mae,\n",
    "            'r2': mlp_r2,\n",
    "            'mape': mlp_mape,\n",
    "            'max_error': mlp_max_err,\n",
    "            'preds': mlp_preds,\n",
    "            'trues': mlp_trues,\n",
    "        },\n",
    "        'KAN': {\n",
    "            'train_losses': kan_train_losses,\n",
    "            'val_losses': kan_val_losses,\n",
    "            'test_loss': kan_test_loss,\n",
    "            'mae': kan_mae,\n",
    "            'r2': kan_r2,\n",
    "            'mape': kan_mape,\n",
    "            'max_error': kan_max_err,\n",
    "            'preds': kan_preds,\n",
    "            'trues': kan_trues,\n",
    "        },\n",
    "        'x_test': x_test,\n",
    "        'y_test': y_test_orig,\n",
    "    }\n",
    "\n",
    "    # ---------------------------\n",
    "    # Plot Training and Validation Losses\n",
    "    # ---------------------------\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(mlp_train_losses, label='MLP Training Loss')\n",
    "    plt.plot(mlp_val_losses, label='MLP Validation Loss')\n",
    "    plt.plot(kan_train_losses, label='KAN Training Loss')\n",
    "    plt.plot(kan_val_losses, label='KAN Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title(f'Training and Validation Losses for {func_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # ---------------------------\n",
    "    # Print Test Metrics for Both Models\n",
    "    # ---------------------------\n",
    "    print(f\"MLP Test MSE Loss: {results[func_name]['MLP']['test_loss']:.6f}\")\n",
    "    print(f\"MLP Test MAE: {results[func_name]['MLP']['mae']:.6f}\")\n",
    "    print(f\"MLP Test R^2 Score: {results[func_name]['MLP']['r2']:.6f}\")\n",
    "    print(f\"MLP Test MAPE: {results[func_name]['MLP']['mape']:.2f}%\")\n",
    "    print(f\"MLP Test Max Error: {results[func_name]['MLP']['max_error']:.6f}\")\n",
    "    \n",
    "    print(f\"KAN Test MSE Loss: {results[func_name]['KAN']['test_loss']:.6f}\")\n",
    "    print(f\"KAN Test MAE: {results[func_name]['KAN']['mae']:.6f}\")\n",
    "    print(f\"KAN Test R^2 Score: {results[func_name]['KAN']['r2']:.6f}\")\n",
    "    print(f\"KAN Test MAPE: {results[func_name]['KAN']['mape']:.2f}%\")\n",
    "    print(f\"KAN Test Max Error: {results[func_name]['KAN']['max_error']:.6f}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Plot Predictions vs. True Function\n",
    "    # ---------------------------\n",
    "    x_test = results[func_name]['x_test']\n",
    "    mlp_preds = results[func_name]['MLP']['preds']\n",
    "    kan_preds = results[func_name]['KAN']['preds']\n",
    "    true_vals = results[func_name]['MLP']['trues']  # True targets are the same for both models.\n",
    "\n",
    "    # Sort the test data for a cleaner plot.\n",
    "    sorted_indices = np.argsort(x_test)\n",
    "    x_test_sorted = x_test[sorted_indices]\n",
    "    true_sorted = true_vals[sorted_indices]\n",
    "    mlp_preds_sorted = mlp_preds[sorted_indices]\n",
    "    kan_preds_sorted = kan_preds[sorted_indices]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(x_test_sorted, true_sorted, 'k-', label='True Function', linewidth=2)\n",
    "    plt.plot(x_test_sorted, mlp_preds_sorted, 'r--', label='MLP Predictions', linewidth=2)\n",
    "    plt.plot(x_test_sorted, kan_preds_sorted, 'b--', label='KAN Predictions', linewidth=2)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.title(f'Function Approximation for {func_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # ---------------------------\n",
    "    # Plot Residuals (Predicted - True)\n",
    "    # ---------------------------\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(x_test, mlp_preds - true_vals, alpha=0.5, label='MLP Residuals')\n",
    "    plt.scatter(x_test, kan_preds - true_vals, alpha=0.5, label='KAN Residuals')\n",
    "    plt.hlines(0, x_test.min(), x_test.max(), colors='k', linestyles='dashed')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.title(f'Residuals for {func_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # ---------------------------\n",
    "    # Plot Histogram of Residuals\n",
    "    # ---------------------------\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(mlp_preds - true_vals, bins=30, alpha=0.5, label='MLP Residuals')\n",
    "    plt.hist(kan_preds - true_vals, bins=30, alpha=0.5, label='KAN Residuals')\n",
    "    plt.xlabel('Residual')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Residual Histogram for {func_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 7. Summary Table of Metrics and Visualization\n",
    "# =============================================================================\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "# Create a summary table that collects error metrics for both models across functions.\n",
    "for func_name in functions.keys():\n",
    "    mlp_metrics = results[func_name]['MLP']\n",
    "    kan_metrics = results[func_name]['KAN']\n",
    "\n",
    "    summary_data.append({\n",
    "        'Function': func_name,\n",
    "        'Model': 'MLP',\n",
    "        'MSE': mlp_metrics['test_loss'],\n",
    "        'MAE': mlp_metrics['mae'],\n",
    "        'R²': mlp_metrics['r2'],\n",
    "        'MAPE (%)': mlp_metrics['mape'],\n",
    "        'Max Error': mlp_metrics['max_error']\n",
    "    })\n",
    "\n",
    "    summary_data.append({\n",
    "        'Function': func_name,\n",
    "        'Model': 'KAN',\n",
    "        'MSE': kan_metrics['test_loss'],\n",
    "        'MAE': kan_metrics['mae'],\n",
    "        'R²': kan_metrics['r2'],\n",
    "        'MAPE (%)': kan_metrics['mape'],\n",
    "        'Max Error': kan_metrics['max_error']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nSummary of Metrics:\")\n",
    "print(summary_df)\n",
    "summary_df.to_csv('model_comparison_summary.csv', index=False)\n",
    "\n",
    "# Visualize the metrics for comparison using Seaborn bar plots.\n",
    "sns.set(style=\"whitegrid\")\n",
    "metrics = ['MSE', 'MAE', 'R²', 'MAPE (%)', 'Max Error']\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.barplot(x='Function', y=metric, hue='Model', data=summary_df)\n",
    "    plt.title(f'Comparison of {metric} between MLP and KAN')\n",
    "    plt.xlabel('Function')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend(title='Model')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envKAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
